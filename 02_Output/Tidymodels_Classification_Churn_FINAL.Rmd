---
title: "Modelling with Tidymodels and Parsnip"
subtitle: "A Tidy Approach to a Classification Problem"
author: "Diego Usai"
date: "22 June 2019"
output:
  html_document:
    theme: readable
    highlight: pygments
    number_sections: false
    fig.align: 'centre'
    toc: true
    toc_float: true
    toc_depth : 4
    font-family: Roboto
    code_folding: none
    keep_md: false
    dpi: 300
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  eval = TRUE,      # TRUE to evaluate every single chunck
  warning = FALSE,  # FALSE to suppress warnings from being shown
  message = FALSE,  # FALSE to avoid package loading messages
  cache = FALSE,    # TRUE to save every single chunck to a folder
  echo = TRUE,      # TRUE for display code in output document
  out.width = "80%",
  out.height = "100%",
  fig.align = "center"
)
```

```{r, include=FALSE}
# turn off locale-specific sorting for messages in English
Sys.setlocale("LC_TIME", "C")
```

## Overview

Recently I have completed an online course focused on _applied data and business science with R_, which introduced me to a couple of new modelling concepts and approaches. One that especially captured my attention is `parsnip` and its attempt to implement a unified modelling and analysis interface (similar to __python's__ `scikit-learn`) to seamlessly access several modelling platforms in R. 

`parsnip` is the brainchild of RStudio's [__Max Khun__](https://twitter.com/topepos) (of `caret` fame) and [__Davis Vaughan__](https://twitter.com/dvaughan32) and forms part of `tidymodels`, a growing ensemble of tools to explore and iterate modelling tasks that shares a common philosophy (and a few libraries) with the `tidyverse`. 

Although there are a number of packages at different stages in their development, I have decided to take `tidymodels` "for a spin", so to speak, and create and execute a "tidy" modelling workflow to tackle a __classification__ problem. My aim is to show how easy it is to fit a simple __logistic regression__ in R's `glm` and quickly switch to a cross-validated __random forest__ using the `ranger` engine by changing only a few lines of code.

For this post in particular I'm focusing on four different libraries from the `tidymodels` suite: `rsample` for data sampling and cross-validation, `recipes` for data preprocessing, `parsnip` for model set up and estimation, and `yardstick` for model assessment.

__Note that__ the focus is on modelling workflow and libraries interaction. For that reason, I am keeping data exploration and feature engineering to a minimum. 
```{r switch off, include=FALSE}
# turn off locale-specific sorting for messages in English
Sys.setlocale("LC_TIME", "C")
```

## Set up

First, I load the packages I need for this analysis.
```{r, packages}
library(tidymodels)
library(skimr)
library(tibble)
```

For this project I am using the [__Telco Customer Churn__](https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/) from [IBM Watson Analytics](https://www.ibm.com/communities/analytics/watson-analytics/), one of IBM Analytics Communities. The data contains 7,043 rows, each representing a customer, and 21 columns for the potential predictors, providing information to forecast customer behaviour and help develop focused customer retention programmes.

`Churn` is the __Dependent Variable__ and shows the customers who left within the last month. The dataset also includes details on the __Services__ that each customer has signed up for, along with __Customer Account__ and __Demographic__ information.
```{r load data}
telco <- readr::read_csv("../00_Data/WA_Fn-UseC_-Telco-Customer-Churn.csv")
```

```{r no histogram, include=FALSE}
skim_with(numeric = list(hist = NULL))
telco %>% skimr::skim()
```

```{r, evaluate = FALSE}
telco %>% 
  skimr::skim()
```

There are a couple of things to notice here: 

+ __customerID__ is a unique identifier for each row. As such it has no descriptive or predictive power and it needs to be removed.

+ Given the relative small number of missing values in __TotalCharges__ (only 11 of them) I am dropping them from the dataset.

```{r}
telco <- 
  telco %>%
  select(-customerID) %>%
  drop_na()
```

## Modelling with `tidymodels`

To show the basic steps in the `tidymodels` framework I am fitting and evaluating a simple __logistic regression__ model.

### Train and test split

`rsample` provides a streamlined way to create a randomised training and test split of the original data.
```{r, collapse = TRUE}
set.seed(seed = 1972) 

train_test_split <-
  rsample::initial_split(
    data = telco,     
    prop = 0.80   
  ) 

train_test_split
```

Of the 7,043 total customers, 5,626 have been assigned to the training set and 1,406 to the test set. I save them as `train_tbl` and `test_tbl`.
```{r}
train_tbl <- train_test_split %>% training() 
test_tbl  <- train_test_split %>% testing() 
```

### A simple recipe

The `recipes` package uses a __cooking metaphor__ to handle all the data preprocessing, like missing values imputation, removing predictors, centring and scaling, one-hot-encoding, and more.

First, I create a `recipe` where I define the transformations I want to apply to my data. In this case I create a simple recipe to change all character variables to factors. 

Then, I _"prep the recipe"_ by mixing the ingredients with `prep`. Here I have included the prep bit in the recipe function for brevity.

```{r}
recipe_simple <- function(dataset) {
  recipe(Churn ~ ., data = dataset) %>%
    step_string2factor(all_nominal(), -all_outcomes()) %>%
    prep(data = dataset)
}
```


__Note that__ in order to avoid _data leakage_ (e.g: transferring information from the train set into the test set), data should be "prepped" using the `train_tbl` only.
```{r}
recipe_prepped <- recipe_simple(dataset = train_tbl)
```

Finally, to continue with the cooking metaphor, I _"bake the recipe"_ to apply all preprocessing to the data sets.
```{r}
train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked  <- bake(recipe_prepped, new_data = test_tbl)
```

### Fit the model  

`parsnip` is a relatively recent addition to the `tidymodels` suite and is probably the one I like best. This package offers a unified API that allows access to several machine learning packages without the need to learn the syntax of each individual one.

With 3 simple steps you can:

+ set the __type of model__ you want to fit (here is a `logistic regression`) and its __mode__ (`classification`)

+ decide which computational __engine__ to use (`glm` in this case) 

+ spell out the exact model specification to __fit__ (I'm using all variables here) and what __data__ to use (the baked train dataset)

```{r simple glm, collapse = TRUE}
logistic_glm <-
  logistic_reg(mode = "classification") %>%
  set_engine("glm") %>%
  fit(Churn ~ ., data = train_baked)
```

If you want to use another engine you can simply switch the `set_engine` argument (for _logistic regression_ you can choose from `glm`, `glmnet`, `stan`, `spark`, and `keras`) and  `parsnip` will take care of changing everything else for you behind the scenes.  

### Performance assessment  

The `yardstick` package provides an easy way to calculate several assessment measures. But before I can evaluate my model's performance, I need to calculate some predictions by passing the `test_baked` data to the `predict` function.
```{r, collapse = TRUE}
predictions_glm <- logistic_glm %>%
  predict(new_data = test_baked) %>%
  bind_cols(test_baked %>% select(Churn))

head(predictions_glm)
```

There are several metrics that can be used to investigate the performance of a classification model but for simplicity I'm only focusing on a selection of them: __accuracy__, __precision__, __recall__ and __F1_Score__. 

All of these measures (and many more) can be derived by the [__Confusion Matrix__](https://en.wikipedia.org/wiki/Confusion_matrix), a table used to describe the performance of a classification model on a set of test data for which the true values are known. 

In and of itself, the confusion matrix is a relatively easy concept to get your head around as is shows the number of _false positives_, _false negatives_, _true positives_, and _true negatives_. However some of the measures that are derived from it may take some reasoning with to fully understand their meaning and use.
```{r, collapse = TRUE}
predictions_glm %>%
  conf_mat(Churn, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)
```

The model's __Accuracy__ is the fraction of predictions the model got right and can be easily calculated by passing the `predictions_glm` to the `metrics` function. However, accuracy is not a very reliable metric as it will provide misleading results if the data set is unbalanced.

With only basic data manipulation and feature engineering the simple logistic model has achieved 80% accuracy. 
```{r, collapse = TRUE}
predictions_glm %>%
  metrics(Churn, .pred_class) %>%
  select(-.estimator) %>%
  filter(.metric == "accuracy") %>%
  kable()
```

__Precision__ shows how sensitive models are to _False Positives_ (i.e. predicting a customer is leaving when he-she is actually staying) whereas __Recall__ looks at how sensitive models are to _False Negatives_ (i.e. forecasting that a customer is staying whilst he-she is in fact leaving).

These are __very relevant business metrics__ because organisations are particularly interested in accurately predicting which customers are truly at risk of leaving so that they can target them with retention strategies. At the same time they want to minimising efforts of retaining customers incorrectly classified as leaving who are instead staying. 
```{r, collapse = TRUE}
tibble(
  "precision" = 
     precision(predictions_glm, Churn, .pred_class) %>%
     select(.estimate),
  "recall" = 
     recall(predictions_glm, Churn, .pred_class) %>%
     select(.estimate)
) %>%
  unnest() %>%
  kable()
```

Another popular performance assessment metric is the [__F1 Score__](https://en.wikipedia.org/wiki/F1_score), which is the harmonic average of the [precision](https://en.wikipedia.org/wiki/Precision_(information_retrieval)) and [recall](https://en.wikipedia.org/wiki/Recall_(information_retrieval)). An F1 score reaches its best value at 1 with perfect _precision_ and _recall_.


```{r, collapse = TRUE}
predictions_glm %>%
  f_meas(Churn, .pred_class) %>%
  select(-.estimator) %>%
  kable()
```


## A Random Forest

This is where the real beauty of `tidymodels` comes into play. Now I can use this tidy modelling framework to fit a __Random Forest__ model with the `ranger` engine.  

### Cross-validation set up

To further refine the model's predictive power, I am implementing a __10-fold cross validation__ using `vfold_cv` from `rsample`, which splits again the initial training data. 

```{r, collapse = TRUE}
cross_val_tbl <- vfold_cv(train_tbl, v = 10)

cross_val_tbl
```

If we take a further look, we should recognise the 5,626 number, which is the total number of observations in the initial `train_tbl`. In each round, 563 observations will in turn be retained from estimation and used to validate the model for that fold.
```{r, collapse = TRUE}
cross_val_tbl$splits %>%
  pluck(1)
```

To avoid confusion and distinguish the _initial train/test splits_ from those used for cross validation, the author of `rsample` __Max Kuhn__ has coined two new terms: the `analysis` and the `assessment` sets. The former is the portion of the train data used to recursively estimate the model, where the latter is the portion used to validate each estimate. 

### Update the recipe

__NOTE__ that a random forest needs _all numeric_ variables to be _centred and scaled_ and all _character/factor_ variables to be _"dummified"_. This is easily done by updating the recipe with these transformations.
```{r}
recipe_rf <- function(dataset) {
  recipe(Churn ~ ., data = dataset) %>%
    step_string2factor(all_nominal(), -all_outcomes()) %>%
    step_dummy(all_nominal(), -all_outcomes()) %>%
    step_center(all_numeric()) %>%
    step_scale(all_numeric()) %>%
    prep(data = dataset)
}
```

### Estimate the model

Switching to another model could not be simpler! All I need to do is to change the __type of model__ to `random_forest` and add its hyper-parameters, change the __set_engine__ argument to `ranger` and I'm ready to go.

I'm bundling all steps into a function that estimates the model across all folds, runs predictions and returns a convenient tibble with all the results. I need to add an extra step before the recipe "prepping" to maps the cross validation splits to the `analysis` and `assessment` functions. This will guide the iterations through the 10 folds.
```{r}
rf_fun <- function(split, id, try, tree) {
   
  analysis_set <- split %>% analysis()
  analysis_prepped <- analysis_set %>% recipe_rf()
  analysis_baked <- analysis_prepped %>% bake(new_data = analysis_set)

  model_rf <-
    rand_forest(
      mode = "classification",
      mtry = try,
      trees = tree
    ) %>%
    set_engine("ranger",
      importance = "impurity"
    ) %>%
    fit(Churn ~ ., data = analysis_baked)

  assessment_set <- split %>% assessment()
  assessment_prepped <- assessment_set %>% recipe_rf()
  assessment_baked <- assessment_prepped %>% bake(new_data = assessment_set)

  tibble(
    "id" = id,
    "truth" = assessment_baked$Churn,
    "prediction" = model_rf %>%
      predict(new_data = assessment_baked) %>%
      unlist()
  )
  
}
```

### Performance assessment

All I have left to do is mapping the formula to a data frame.
```{r, collapse = TRUE}
pred_rf <- map2_df(
  .x = cross_val_tbl$splits,
  .y = cross_val_tbl$id,
  ~ rf_fun(split = .x, id = .y, try = 3, tree = 200)
)

head(pred_rf)
```

I've found that `yardstick` has a very handy confusion matrix `summary` function, which returns an array of __13 different metrics__ but in this case I want to see the four I used for the `glm` model.
```{r, cache = TRUE}
pred_rf %>%
  conf_mat(truth, prediction) %>%
  summary() %>%
  select(-.estimator) %>%
  filter(.metric %in%
    c("accuracy", "precision", "recall", "f_meas")) %>%
  kable()
```


The `random forest` model is performing in par with the simple `logistic regression`. Given the very basic feature engineering that I've carried out, there is scope to further improve the model but this is beyond the scope of this post.

## Closing considerations 

One of the great advantage of `tidymodels` is the flexibility and ease of access to every phase of the analysis workflow. Creating the modelling pipeline is a breeze and you can easily re-use the initial framework by changing model type with `parsnip` and data pre-processing with `recipes` and in no time you're ready to check your new model's performance with `yardstick`.

In any analysis you would typically audit several models and `parsnip` frees you up from having to learn the unique syntax of every modelling engine so that you can focus on finding the best solution for the problem at hand.


### Code repository
The full R code can be found on [my GitHub profile](https://github.com/DiegoUsaiUK/Classification_Churn_with_Parsnip)


### References
* Big thanks to _Bruno Rodrigues_ for the article that provided the inspiration for the big evaluation formula [A tutorial on tidy cross-validation with R](https://www.brodrigues.co/blog/2018-11-25-tidy_cv/)
* More thanks to _Benjamin Sorensen_ for his thoughtful piece on [Modeling with `parsnip` and `tidymodels`](https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/)
* For an introduction to [`parsnip`](https://www.tidyverse.org/articles/2018/11/parsnip-0-0-1/)
